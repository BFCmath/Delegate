# LLM-SLM Communication Flow Verification

## üîç Diagnostic Results

### Test Data Analysis
Analyzed your `results_router.json` with 10 problems:

| Metric | Result | Status |
|--------|--------|--------|
| SLM called | 10/10 (100%) | ‚úÖ Working |
| SLM generated response | 10/10 (100%) | ‚úÖ Working |
| LLM responded after SLM | 6/10 (60%) | ‚ùå **PROBLEM** |
| Empty final responses | 4/10 (40%) | ‚ùå **CRITICAL** |

### ‚ùå Confirmed Issue

**The LLM IS receiving the SLM response, but NOT generating output.**

Looking at problem #2 (Painters) as an example:

```
‚úÖ SLM Call #1:
    Input: "A team of 4 painters worked on a mansion for 3/8ths of a day..."
    Output: "To determine the total hours of work each painter put in, we need to follow these steps..."
    Length: 1234 chars  ‚Üê SLM GENERATED RESPONSE

‚úÖ LLM Turn 0 (Initial):
    Function call: slm_help('A team of 4 painters...')  ‚Üê LLM CALLED SLM

‚ùå LLM Turn 1 (After SLM):
    Input: "[Function responses: 1]"  ‚Üê LLM RECEIVED SLM RESPONSE
    Output: EMPTY!  ‚Üê LLM DIDN'T GENERATE ANYTHING
```

## üîß Code Flow Verification

### Step-by-Step Trace

**File: `experiments/router_agent.py`**

1. **SLM generates response** (Line 75-88):
   ```python
   gen = tok.batch_decode(out[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)[0]
   # gen = "To determine the total hours... \\boxed{47.25}"
   
   result = f"The calculation result is: {answer}\n\nNow provide your final answer using this result."
   # result = "The calculation result is: 47.25\n\nNow provide your final answer..."
   ```
   ‚úÖ **Confirmed**: SLM creates `result` string

2. **Pack into FunctionResponse** (Lines 211-221):
   ```python
   function_responses.append(
       genai.protos.Part(
           function_response=genai.protos.FunctionResponse(
               name="slm_help",
               response={"result": result}  # Contains SLM output
           )
       )
   )
   ```
   ‚úÖ **Confirmed**: Result is packed correctly

3. **Send to LLM** (Line 224):
   ```python
   response = await asyncio.to_thread(chat.send_message, function_responses)
   ```
   ‚úÖ **Confirmed**: Sent to Gemini API

4. **Extract LLM response** (Lines 237-247):
   ```python
   if response.candidates and response.candidates[0].content.parts:
       for part in response.candidates[0].content.parts:
           if hasattr(part, 'text') and part.text:
               turn_output += part.text  # ‚Üê Should contain LLM's answer
   ```
   ‚ùå **Problem**: `part.text` is often empty!

## üß™ Debug Logging Added

Enhanced `router_agent.py` with detailed logging:

```python
# Line 212: Before sending to LLM
print(f"[DEBUG] Sending to LLM: result={result[:100]}...")

# Line 227: After LLM responds
print(f"[DEBUG] LLM Response received: candidates={len(response.candidates)}")

# Line 238: Check response parts
print(f"[DEBUG] LLM Response parts count: {len(response.candidates[0].content.parts)}")

# Line 242: Check text content
print(f"[DEBUG] LLM Generated text: {part.text[:100]}...")

# Line 252: Flag empty responses
if not turn_output:
    print(f"[DEBUG] ‚ùå LLM generated EMPTY output after receiving SLM result!")
    print(f"[DEBUG] finish_reason: {response.candidates[0].finish_reason}")
```

## üî¨ Test With Debug Logs

### Run test with enhanced logging:

```bash
python run_router_only.py --samples 5
```

### Expected Debug Output (Working Case):
```
[TOOL] slm_help: (490 - 150) * 194
[SLM] Answer: 65960 (8.43s)
[TRACKER] Logged: 8.43s, 52‚Üí264 tokens
[DEBUG] Sending to LLM: result=The calculation result is: 65960

Now provide your final answer using this result....
[DEBUG] LLM Response received: candidates=1
[DEBUG] LLM Response parts count: 1
[DEBUG] LLM Generated text: The difference in length between Lewis' street and Monica's street is 490 meters - 150 meters =...
‚úì | 9.78s | tools=1 | 457‚Üí114 tokens
```

### Expected Debug Output (Problem Case):
```
[TOOL] slm_help: A team of 4 painters worked on a mansion for 3/8ths of a day...
[SLM] Answer: 47.25 (13.11s)
[TRACKER] Logged: 13.11s, 74‚Üí381 tokens
[DEBUG] Sending to LLM: result=The calculation result is: 47.25

Now provide your final answer using this result....
[DEBUG] LLM Response received: candidates=1
[DEBUG] LLM Response parts count: 0  ‚Üê NO PARTS!
[DEBUG] ‚ö†Ô∏è  No response parts from LLM!
[DEBUG] ‚ùå LLM generated EMPTY output after receiving SLM result!
[DEBUG] finish_reason: 1  ‚Üê or possibly 2 (MAX_TOKENS), 3 (SAFETY), 4 (RECITATION)
‚úó | 14.29s | tools=1 | 458‚Üí52 tokens
```

## üéØ Root Causes Identified

### 1. Prompt Issue (v1.0) - **FIXED in v2.0**
**Problem**: Old prompt didn't explicitly tell LLM to respond after tool call

**v1.0 Prompt** (BAD):
```
"Your task is to deconstruct the problem into a single, complete mathematical 
expression and then call the `slm_help` tool to solve it."
```
‚Üí No instruction about what to do AFTER calling tool!

**v2.0 Prompt** (GOOD):
```
## Your Workflow:
...
4. **Receive the result** and immediately use it
5. **Provide final answer** in \boxed{answer} format

## Critical:
- After receiving the tool result, ALWAYS provide your final answer
- Never leave the response empty
```

### 2. SLM Response Format - **IMPROVED in v2.0**
**v1.0 Format** (Less Clear):
```
"CALCULATION COMPLETE: The answer is 47.25. Use this directly."
```

**v2.0 Format** (Explicit):
```
"The calculation result is: 47.25

Now provide your final answer using this result."
```
‚Üí Contains explicit call-to-action!

### 3. Possible Gemini API Issues

From the debug logs, check `finish_reason`:
- `1` (STOP) = Normal completion ‚Üí LLM chose not to generate
- `2` (MAX_TOKENS) = Hit token limit ‚Üí Need more tokens
- `3` (SAFETY) = Safety filter triggered ‚Üí Content issue
- `4` (RECITATION) = Recitation filter ‚Üí Copying issue

## üìä Verification Steps

### Step 1: Run Diagnostic
```bash
python diagnose_llm_slm_flow.py results_router.json
```

### Step 2: Check Debug Logs
Look for:
- ‚úÖ `[DEBUG] Sending to LLM:` - Confirms SLM result sent
- ‚úÖ `[DEBUG] LLM Response received: candidates=1` - LLM got it
- ‚ùå `[DEBUG] LLM Response parts count: 0` - LLM didn't generate
- ‚ùå `[DEBUG] ‚ùå LLM generated EMPTY output` - Confirmed issue

### Step 3: Check finish_reason
```python
import json
data = json.load(open('results_router.json'))

for r in data['results']:
    if not r['prediction'] or 'No response' in r['prediction']:
        print(f"Problem: {r['problem_id']}")
        # Check if we logged finish_reason in debug output
```

### Step 4: Verify Prompt Version
```bash
python -c "from prompts import PROMPT_VERSION, ROUTER_INSTRUCTIONS_EXPERIMENT; print(f'Version: {PROMPT_VERSION}'); print('Has workflow:', 'Receive the result' in ROUTER_INSTRUCTIONS_EXPERIMENT)"
```

Expected output:
```
Version: 2.0.0
Has workflow: True
```

## ‚úÖ Expected Improvements (v2.0)

With the improved prompts and response format:

| Metric | v1.0 | v2.0 Target | Change |
|--------|------|-------------|--------|
| LLM responds after SLM | 60% ‚ùå | ~95-100% ‚úÖ | +35-40% |
| Empty responses | 40% ‚ùå | ~0-5% ‚úÖ | -35-40% |
| Overall accuracy | ~50% | ~65-70% | +15-20% |

## üîß If Issues Persist

### Check 1: Token Limits
If `finish_reason = 2 (MAX_TOKENS)`:

```python
# In experiments/router_agent.py, line 152
model = key_manager.get_model(
    model_name="gemini-2.5-flash-lite",
    tools=[slm_help_tool],
    system_instruction=INSTRUCTIONS,
    generation_config=genai.types.GenerationConfig(
        max_output_tokens=2048,  # Increase from default 1024
    )
)
```

### Check 2: Safety Filters
If `finish_reason = 3 (SAFETY)`:
- Check if SLM output contains sensitive content
- May need to filter SLM response before sending to LLM

### Check 3: Alternative Prompt
Try multistep version:
```python
from prompts import get_router_instructions
INSTRUCTIONS = get_router_instructions("multistep")
```

## üìà Monitoring

### Create monitoring script:
```python
# monitor_llm_response_rate.py
import json
import sys

def monitor(results_file):
    data = json.load(open(results_file))
    results = data['results']
    
    responded = sum(1 for r in results 
                   if r.get('llm_conversation', [[]])[1:] 
                   and r['llm_conversation'][1].get('output'))
    
    total = len(results)
    rate = responded / total if total else 0
    
    print(f"LLM Response Rate: {responded}/{total} ({rate*100:.1f}%)")
    
    if rate < 0.9:
        print("‚ö†Ô∏è  Response rate below 90% - check prompts")
    else:
        print("‚úÖ Response rate healthy")

if __name__ == "__main__":
    monitor(sys.argv[1] if len(sys.argv) > 1 else 'results_router.json')
```

## üéØ Summary

### Confirmed Facts:
1. ‚úÖ SLM is being called (100%)
2. ‚úÖ SLM is generating responses (100%)
3. ‚úÖ SLM responses are being sent to LLM (100%)
4. ‚ùå **LLM is NOT always generating output (only 60%)**

### Root Cause:
**Prompt Engineering Issue** - LLM wasn't explicitly told to:
- Respond after receiving tool results
- Never leave response empty
- Always provide final answer

### Fix:
- ‚úÖ Updated prompt with explicit instructions (v2.0)
- ‚úÖ Added call-to-action in SLM response format
- ‚úÖ Added debug logging to track flow
- ‚úÖ Created diagnostic tools

### Verification:
Run the diagnostic and look for the `[DEBUG]` messages to confirm the flow is working.

---

**Next Steps:**
1. Test with new prompts: `python run_router_only.py --samples 10`
2. Run diagnostic: `python diagnose_llm_slm_flow.py results_router.json`
3. Check for `[DEBUG]` messages showing empty LLM responses
4. If issues persist, check `finish_reason` values

