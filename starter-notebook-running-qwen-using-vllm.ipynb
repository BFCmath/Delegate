{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9760679,"sourceType":"datasetVersion","datasetId":5977063}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Our goal in this notebook\n\n* Run a Qwen large language model using vLLM in a Kaggle Notebook\n* Utilize Kaggle provided GPU for the notebook \n\n\nMake sure to Keep Internet On for this notebook","metadata":{}},{"cell_type":"markdown","source":"Step 1: Attach vLLM wheels under \"Datasets\" using the 'Add Input' option in the Kaggle notebook\n\nStep 2: Switch on GPU T4x2 Accelerator\n\nNow, let's start our notebook session and begin coding","metadata":{}},{"cell_type":"code","source":"!pip install --no-index --find-links=/kaggle/input/vllm-0-6-3-post1-wheels torchvision==0.19.1\n!pip install --no-index --find-links=/kaggle/input/vllm-0-6-3-post1-wheels vllm","metadata":{"execution":{"iopub.status.busy":"2024-11-17T00:53:24.844005Z","iopub.execute_input":"2024-11-17T00:53:24.844373Z","iopub.status.idle":"2024-11-17T00:56:16.497880Z","shell.execute_reply.started":"2024-11-17T00:53:24.844335Z","shell.execute_reply":"2024-11-17T00:56:16.496843Z"},"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"List of all Qwen models can be found from it's official website: https://qwen.readthedocs.io/en/latest/getting_started/concepts.html\n\nQwen: the language models\n* Qwen: 1.8B, 7B, 14B, and 72B models\n* Qwen1.5: 0.5B, 1.8B, 4B, 14BA2.7B, 7B, 14B, 32B, 72B, and 110B models\n* Qwen2: 0.5B, 1.5B, 7B, 57A14B, and 72B models\n* Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B models\n\nQwen-VL: the vision-language models\n* Qwen-VL: 7B-based models\n* Qwen2-VL: 2B, 7B, and 72B-based models\n* Qwen-Audio: the audio-language models\n\nQwen-Audio: 7B-based model\n* Qwen2-Audio: 7B-based models\n\nCodeQwen/Qwen-Coder: the language models for coding\n* CodeQwen1.5: 7B models\n* Qwen2.5-Coder: 7B models\n\nQwen-Math: the language models for mathematics\n* Qwen2-Math: 1.5B, 7B, and 72B models\n* Qwen2.5-Math: 1.5B, 7B, and 72B models","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams","metadata":{"execution":{"iopub.status.busy":"2024-11-17T01:01:23.619947Z","iopub.execute_input":"2024-11-17T01:01:23.620792Z","iopub.status.idle":"2024-11-17T01:01:23.624863Z","shell.execute_reply.started":"2024-11-17T01:01:23.620752Z","shell.execute_reply":"2024-11-17T01:01:23.623953Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Qwen's HuggingFace page: https://huggingface.co/Qwen","metadata":{}},{"cell_type":"code","source":"# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")","metadata":{"execution":{"iopub.status.busy":"2024-11-17T01:03:22.963175Z","iopub.execute_input":"2024-11-17T01:03:22.963576Z","iopub.status.idle":"2024-11-17T01:03:25.528687Z","shell.execute_reply.started":"2024-11-17T01:03:22.963538Z","shell.execute_reply":"2024-11-17T01:03:25.527850Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Input the model name or path. Can be GPTQ or AWQ models.\nllm = LLM(model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n         dtype=\"half\")\n\n#ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. \n# Your b'Tesla T4' GPU has compute capability 7.5. You can use float16 instead by explicitly setting the`dtype` flag in CLI, for example: --dtype=half.","metadata":{"execution":{"iopub.status.busy":"2024-11-17T01:05:06.340983Z","iopub.execute_input":"2024-11-17T01:05:06.341639Z","iopub.status.idle":"2024-11-17T01:06:12.357830Z","shell.execute_reply.started":"2024-11-17T01:05:06.341600Z","shell.execute_reply":"2024-11-17T01:06:12.356814Z"},"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)","metadata":{"execution":{"iopub.status.busy":"2024-11-17T01:06:23.310414Z","iopub.execute_input":"2024-11-17T01:06:23.311313Z","iopub.status.idle":"2024-11-17T01:06:23.315647Z","shell.execute_reply.started":"2024-11-17T01:06:23.311270Z","shell.execute_reply":"2024-11-17T01:06:23.314719Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare your prompts\nprompt = \"Tell me something about large language models.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-17T01:06:32.908703Z","iopub.execute_input":"2024-11-17T01:06:32.909615Z","iopub.status.idle":"2024-11-17T01:06:32.941726Z","shell.execute_reply.started":"2024-11-17T01:06:32.909554Z","shell.execute_reply":"2024-11-17T01:06:32.940825Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate outputs\noutputs = llm.generate([text], sampling_params)","metadata":{"execution":{"iopub.status.busy":"2024-11-17T01:06:40.480051Z","iopub.execute_input":"2024-11-17T01:06:40.480825Z","iopub.status.idle":"2024-11-17T01:06:42.635778Z","shell.execute_reply.started":"2024-11-17T01:06:40.480789Z","shell.execute_reply":"2024-11-17T01:06:42.634889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the outputs.\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-17T01:06:46.247446Z","iopub.execute_input":"2024-11-17T01:06:46.248166Z","iopub.status.idle":"2024-11-17T01:06:46.253585Z","shell.execute_reply.started":"2024-11-17T01:06:46.248124Z","shell.execute_reply":"2024-11-17T01:06:46.252601Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"References:\n1. https://www.kaggle.com/code/cooleel/starter-zero-shot-data-extraction\n2. https://docs.vllm.ai/en/stable/dev/offline_inference/llm.html\n3. https://qwen.readthedocs.io/en/latest/deployment/vllm.html","metadata":{}}]}